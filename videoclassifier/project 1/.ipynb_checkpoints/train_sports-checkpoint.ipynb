{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "91914ca0-4bb7-4c92-b13d-dd20ff2ec0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74b5b85b-a375-4f8e-817f-9935fe8126ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ced0c5a-922f-4393-bc00-1fae795f48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3d45e11-434b-40e3-b2d6-88626f14f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e4bd372-3ebf-46c8-976d-6b203e9da7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"C:/Users/Dell/Desktop/videoclassifier/data\"\n",
    "outputmodel = \"C:/Users/Dell/Desktop/videoclassifier/video_classification_model/videoClassificationModel.keras\"\n",
    "outputlabelbinarizer = \"C:/Users/Dell/Desktop/videoclassifier/model/videoClassificationBinarizer\"\n",
    "epoch = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "780ef7de-ff76-42bf-beb6-7a90a7ca1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are being loaded...\n"
     ]
    }
   ],
   "source": [
    "Sports_Labels = set(['boxing', 'swimming','table_tennis'])\n",
    "print(\"Images are being loaded...\")\n",
    "pathToImages = list(paths.list_images(datapath))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for images in pathToImages:\n",
    "    label = images.split(os.path.sep)[-2]\n",
    "    if label not in Sports_Labels:\n",
    "        continue\n",
    "    image = cv2.imread(images)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,(244,244))\n",
    "    data.append(image)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc2401b9-e045-41af-9c93-20088c24223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "#hot encoded values as 0,1,2\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec706198-425a-4246-8f4d-e485a04d694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "25d4127f-346e-4192-8047-cee8292b4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mean values for normalization\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "\n",
    "# Define a function to normalize images\n",
    "def preprocess_image(img):\n",
    "    return img - mean\n",
    "\n",
    "# Initialize ImageDataGenerator for training with augmentations\n",
    "traininAugmentation = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    preprocessing_function=preprocess_image  # Apply normalization\n",
    ")\n",
    "\n",
    "# Initialize ImageDataGenerator for validation without augmentations\n",
    "validationAugmentation = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image  # Apply normalization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce74ad97-4d38-449b-a652-f4817c582c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf2ad21f-a54d-4cba-9ae6-9a18a3e7bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = ResNet50(weights=\"imagenet\", include_top= False, input_tensor=Input(shape=(244,244,3)))\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7,7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(len(lb.classes_), activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for basemodelLayers in baseModel.layers:\n",
    "    basemodelLayers.trainabble = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a4f52436-8f45-4838-a859-792a12e7da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a48515da-eb8a-44f5-a079-fb3cc9572edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "opt = SGD(learning_rate=0.0001, momentum=0.9, decay=1e-4/epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "025a8289-b011-420a-afd8-3c09712371c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "036ff3da-00ba-4a9c-b701-cde2d47f1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 10s/step - accuracy: 0.4500 - loss: 1.3086 - val_accuracy: 0.8613 - val_loss: 0.4408\n",
      "Epoch 2/25\n",
      "\u001b[1m 1/49\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 594ms/step - accuracy: 0.0000e+00 - loss: 1.2900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.0000e+00 - loss: 1.2900 - val_accuracy: 0.6364 - val_loss: 0.5536\n",
      "Epoch 3/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 11s/step - accuracy: 0.7103 - loss: 0.6743 - val_accuracy: 0.9004 - val_loss: 0.2771\n",
      "Epoch 4/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.7812 - loss: 0.4990 - val_accuracy: 1.0000 - val_loss: 0.1853\n",
      "Epoch 5/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 11s/step - accuracy: 0.8419 - loss: 0.4442 - val_accuracy: 0.9297 - val_loss: 0.2072\n",
      "Epoch 6/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8750 - loss: 0.3644 - val_accuracy: 0.8182 - val_loss: 0.2476\n",
      "Epoch 7/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m712s\u001b[0m 15s/step - accuracy: 0.8444 - loss: 0.3920 - val_accuracy: 0.9434 - val_loss: 0.1745\n",
      "Epoch 8/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 43ms/step - accuracy: 0.9375 - loss: 0.2467 - val_accuracy: 1.0000 - val_loss: 0.1685\n",
      "Epoch 9/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 14s/step - accuracy: 0.8959 - loss: 0.2910 - val_accuracy: 0.9570 - val_loss: 0.1523\n",
      "Epoch 10/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 33ms/step - accuracy: 0.9375 - loss: 0.1533 - val_accuracy: 1.0000 - val_loss: 0.0981\n",
      "Epoch 11/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 14s/step - accuracy: 0.9018 - loss: 0.2694 - val_accuracy: 0.9570 - val_loss: 0.1322\n",
      "Epoch 12/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.8750 - loss: 0.2779 - val_accuracy: 0.9091 - val_loss: 0.2549\n",
      "Epoch 13/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 15s/step - accuracy: 0.9159 - loss: 0.2298 - val_accuracy: 0.9570 - val_loss: 0.1229\n",
      "Epoch 14/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.9688 - loss: 0.1168 - val_accuracy: 1.0000 - val_loss: 0.0476\n",
      "Epoch 15/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 14s/step - accuracy: 0.9312 - loss: 0.2042 - val_accuracy: 0.9570 - val_loss: 0.1159\n",
      "Epoch 16/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.9375 - loss: 0.1703 - val_accuracy: 0.9091 - val_loss: 0.2042\n",
      "Epoch 17/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 14s/step - accuracy: 0.9287 - loss: 0.2057 - val_accuracy: 0.9570 - val_loss: 0.1103\n",
      "Epoch 18/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.8750 - loss: 0.2896 - val_accuracy: 1.0000 - val_loss: 0.0259\n",
      "Epoch 19/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 15s/step - accuracy: 0.9502 - loss: 0.1729 - val_accuracy: 0.9609 - val_loss: 0.1032\n",
      "Epoch 20/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 37ms/step - accuracy: 0.9688 - loss: 0.1647 - val_accuracy: 0.9091 - val_loss: 0.3237\n",
      "Epoch 21/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 15s/step - accuracy: 0.9440 - loss: 0.1706 - val_accuracy: 0.9570 - val_loss: 0.1007\n",
      "Epoch 22/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1022 - val_accuracy: 1.0000 - val_loss: 0.0274\n",
      "Epoch 23/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 14s/step - accuracy: 0.9510 - loss: 0.1371 - val_accuracy: 0.9570 - val_loss: 0.1015\n",
      "Epoch 24/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 34ms/step - accuracy: 0.9062 - loss: 0.2107 - val_accuracy: 0.9091 - val_loss: 0.1054\n",
      "Epoch 25/25\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 15s/step - accuracy: 0.9523 - loss: 0.1469 - val_accuracy: 0.9629 - val_loss: 0.0944\n"
     ]
    }
   ],
   "source": [
    "History = model.fit(\n",
    "    traininAugmentation.flow(X_train, Y_train, batch_size=32),\n",
    "    steps_per_epoch=len(X_train)//32,\n",
    "    validation_data=validationAugmentation.flow(X_test, Y_test),\n",
    "    validation_steps=len(X_test)//32,\n",
    "    epochs=epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6071bbf4-f387-439b-9f3e-40ea96664de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8721d48d-2965-4500-92fd-46234f347d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at: C:/Users/Dell/Desktop/videoclassifier/video_classification_model/videoClassificationModel.keras\n",
      "Binarizer saved successfully at: C:/Users/Dell/Desktop/videoclassifier/video_classification_model/videoClassificationBinarizer.pickle\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(outputmodel), exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "try:\n",
    "    model.save(outputmodel)\n",
    "    print(f\"Model saved successfully at: {outputmodel}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Define the path for the label binarizer\n",
    "binarizer_path = r\"C:/Users/Dell/Desktop/videoclassifier/video_classification_model/videoClassificationBinarizer.pickle\"\n",
    "\n",
    "# Create the directory for the binarizer if it doesn't exist\n",
    "os.makedirs(os.path.dirname(binarizer_path), exist_ok=True)\n",
    "\n",
    "# Save the label binarizer\n",
    "try:\n",
    "    with open(binarizer_path, \"wb\") as lbinarizer:\n",
    "        pickle.dump(lb, lbinarizer)\n",
    "        print(f\"Binarizer saved successfully at: {binarizer_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving binarizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec18377-c92c-4f78-88b2-0a647ebfa90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
